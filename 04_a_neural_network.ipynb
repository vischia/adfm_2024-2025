{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vischia/adfm_2024-2025/blob/master/04a_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Datos en Física Moderna 2024-2025\n",
    "@Pietro Vischia (pietro.vischia@cern.ch)\n",
    "\n",
    "\n",
    "## Pietro Vischia (Universidad de Oviedo and ICTEA), pietro.vischia@cern.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few global settings\n",
    "\n",
    "- If you run on colab, set to `True` the relevant environmental variable.\n",
    "\n",
    "This tutorial will assume you have a few GB free wherever you are running (locally on your machine, or in your google drive account)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "runOnColab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runOnColab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd \"/content/drive/MyDrive/adfm_2024-2025/\"\n",
    "\n",
    "    !pip install livelossplot shap uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import socket\n",
    "import json\n",
    "import pickle\n",
    "import gzip\n",
    "import copy\n",
    "import array\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions as recfunc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.optimize import newton\n",
    "from scipy.stats import norm\n",
    "\n",
    "import uproot\n",
    "\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.inspection import permutation_importance\n",
    "try:\n",
    "    # See #1137: this allows compatibility for scikit-learn >= 0.24\n",
    "    from sklearn.utils import safe_indexing\n",
    "except ImportError:\n",
    "    from sklearn.utils import _safe_indexing\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have programmed a neural network from scratch. Now you will use the library `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchinfo\n",
    "import torchinfo\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "dyld[69487]: Library not loaded: @rpath/libintl.8.dylib\n",
      "  Referenced from: <0714F760-4165-3CE5-82EB-B7E0AC670F9D> /opt/miniconda3/envs/pv_machine_learning_school/bin/wget\n",
      "  Reason: tried: '/opt/miniconda3/envs/pv_machine_learning_school/bin/../lib/libintl.8.dylib' (no such file), '/opt/miniconda3/envs/pv_machine_learning_school/bin/../lib/libintl.8.dylib' (no such file), '/usr/local/lib/libintl.8.dylib' (no such file), '/usr/lib/libintl.8.dylib' (no such file, not in dyld cache)\n",
      "/opt/homebrew/bin/bash: línea 1: 69487 Abort trap: 6           wget https://www.hep.uniovi.es/vischia/lisbon_ml_school/lisbon_ml_school_tth.tar.gz\n",
      "tar: Error opening archive: Failed to open 'lisbon_ml_school_tth.tar.gz'\n",
      "rm: lisbon_ml_school_tth.tar.gz: No such file or directory\n",
      "/Users/vischia/bureaucracy/uniovi/docencia/Analisis de Datos en Fisica Moderna/2024-2025/adfm_2024-2025\n"
     ]
    }
   ],
   "source": [
    "# This line ownloads the data only if you haven't done so yet\n",
    "\n",
    "if not os.path.isfile(\"data/signal_blind20.root\"): \n",
    "    !mkdir data; cd data/; wget https://www.hep.uniovi.es/vischia/lisbon_ml_school/lisbon_ml_school_tth.tar.gz; tar xzvf lisbon_ml_school_tth.tar.gz; rm lisbon_ml_school_tth.tar.gz; cd -;\n",
    "else:\n",
    "    print(\"Data were already downloaded, I am not downloading them again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is made of three files, one per background (`background_1` is ttW, `background_2`is Drell-Yan.\n",
    "The ttH signal events file corresponds to only a percentage of the full data set: the rest is used in the data challenge (for schools where I offer a data challenge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "sig = uproot.open('data/signal_blind20.root')['Friends'].arrays(library=\"pd\")\n",
    "bk1 = uproot.open('data/background_1.root')['Friends'].arrays(library=\"pd\")\n",
    "bk2 = uproot.open('data/background_2.root')['Friends'].arrays(library=\"pd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data inspection\n",
    "\n",
    "See the notebook `02_data_inspection.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bkg1 shape (556648, 35)\n",
      "bkg2 shape (1034904, 35)\n",
      "bkg1+bkg2 shape (1591552, 35)\n",
      " Signal shape (299287, 36)\n",
      " Bkg shape (1591552, 35)\n",
      " Data shape (1890839, 36)\n",
      "Index(['index', 'Hreco_Lep0_pt', 'Hreco_Lep1_pt', 'Hreco_Lep2_pt',\n",
      "       'Hreco_HadTop_pt', 'Hreco_All5_Jets_pt', 'Hreco_More5_Jets_pt',\n",
      "       'Hreco_Jets_plus_Lep_pt', 'Hreco_Lep0_eta', 'Hreco_Lep1_eta',\n",
      "       'Hreco_Lep2_eta', 'Hreco_HadTop_eta', 'Hreco_All5_Jets_eta',\n",
      "       'Hreco_More5_Jets_eta', 'Hreco_Jets_plus_Lep_eta', 'Hreco_Lep0_phi',\n",
      "       'Hreco_Lep1_phi', 'Hreco_Lep2_phi', 'Hreco_HadTop_phi',\n",
      "       'Hreco_All5_Jets_phi', 'Hreco_More5_Jets_phi',\n",
      "       'Hreco_Jets_plus_Lep_phi', 'Hreco_Lep0_mass', 'Hreco_Lep1_mass',\n",
      "       'Hreco_Lep2_mass', 'Hreco_HadTop_mass', 'Hreco_All5_Jets_mass',\n",
      "       'Hreco_More5_Jets_mass', 'Hreco_Jets_plus_Lep_mass', 'Hreco_TopScore',\n",
      "       'Hreco_met', 'Hreco_met_phi', 'Hreco_HTXS_Higgs_pt',\n",
      "       'Hreco_HTXS_Higgs_y', 'Hreco_evt_tag', 'label'],\n",
      "      dtype='object')\n",
      " Data shape (1103770, 24)\n",
      "Index(['Hreco_Lep0_pt', 'Hreco_Lep1_pt', 'Hreco_HadTop_pt',\n",
      "       'Hreco_All5_Jets_pt', 'Hreco_Jets_plus_Lep_pt', 'Hreco_Lep0_eta',\n",
      "       'Hreco_Lep1_eta', 'Hreco_HadTop_eta', 'Hreco_All5_Jets_eta',\n",
      "       'Hreco_Jets_plus_Lep_eta', 'Hreco_Lep0_phi', 'Hreco_Lep1_phi',\n",
      "       'Hreco_HadTop_phi', 'Hreco_All5_Jets_phi', 'Hreco_Jets_plus_Lep_phi',\n",
      "       'Hreco_Lep0_mass', 'Hreco_Lep1_mass', 'Hreco_HadTop_mass',\n",
      "       'Hreco_All5_Jets_mass', 'Hreco_Jets_plus_Lep_mass', 'Hreco_TopScore',\n",
      "       'Hreco_met', 'Hreco_met_phi', 'label'],\n",
      "      dtype='object')\n",
      "In this dataframe we finally have 206067 signal and 897703 background events\n",
      "Sanity check: look for NAN numbers in any of the rows or columns\n",
      "Hreco_Lep0_pt               False\n",
      "Hreco_Lep1_pt               False\n",
      "Hreco_HadTop_pt             False\n",
      "Hreco_All5_Jets_pt          False\n",
      "Hreco_Jets_plus_Lep_pt      False\n",
      "Hreco_Lep0_eta              False\n",
      "Hreco_Lep1_eta              False\n",
      "Hreco_HadTop_eta            False\n",
      "Hreco_All5_Jets_eta         False\n",
      "Hreco_Jets_plus_Lep_eta     False\n",
      "Hreco_Lep0_phi              False\n",
      "Hreco_Lep1_phi              False\n",
      "Hreco_HadTop_phi            False\n",
      "Hreco_All5_Jets_phi         False\n",
      "Hreco_Jets_plus_Lep_phi     False\n",
      "Hreco_Lep0_mass             False\n",
      "Hreco_Lep1_mass             False\n",
      "Hreco_HadTop_mass           False\n",
      "Hreco_All5_Jets_mass        False\n",
      "Hreco_Jets_plus_Lep_mass    False\n",
      "Hreco_TopScore              False\n",
      "Hreco_met                   False\n",
      "Hreco_met_phi               False\n",
      "label                       False\n",
      "dtype: bool\n",
      "There are NaN-filled elements: False\n",
      "data shape (1103770, 24)\n",
      "input feature shape (1103770, 23)\n",
      "label (=target) shape (1103770,)\n"
     ]
    }
   ],
   "source": [
    "# Create a new column 'label' and set its value to 1 or 0 for all rows (=events)\n",
    "sig['label'] = 1 \n",
    "bk1['label'] = 0\n",
    "bk2['label'] = 0\n",
    "\n",
    "# Merge the two backgrounds into one dataframe\n",
    "bkg = pd.concat([bk1, bk2])\n",
    "\n",
    "print(f\"bkg1 shape {bk1.shape}\")\n",
    "print(f\"bkg2 shape {bk2.shape}\")\n",
    "print(f\"bkg1+bkg2 shape {bkg.shape}\")\n",
    "\n",
    "# Merge the signal and background into one dataframe\n",
    "print(f\" Signal shape {sig.shape}\")\n",
    "print(f\" Bkg shape {bkg.shape}\")\n",
    "\n",
    "data = pd.concat([sig,bkg])\n",
    "\n",
    "print(f\" Data shape {data.shape}\")\n",
    "print(data.columns)\n",
    "\n",
    "# Filter data\n",
    "data=data[data['Hreco_Lep2_pt']==-99]\n",
    "# Drop unneeded features\n",
    "data = data.drop([\"index\",\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \n",
    "                  \"Hreco_evt_tag\",\"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\", \"Hreco_More5_Jets_pt\", \"Hreco_More5_Jets_eta\", \"Hreco_More5_Jets_phi\", \"Hreco_More5_Jets_mass\",], axis=1 )\n",
    "\n",
    "\n",
    "print(f\" Data shape {data.shape}\")\n",
    "print(data.columns)\n",
    "print(f\"In this dataframe we finally have {data[data['label']==1].shape[0]} signal and {data[data['label']==0].shape[0]} background events\")\n",
    "print(\"Sanity check: look for NAN numbers in any of the rows or columns\")\n",
    "print(data.isna().any())\n",
    "\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head(10)\n",
    "\n",
    "\n",
    "print(\"There are NaN-filled elements:\", data.isna().any().any())\n",
    "\n",
    "X = data.drop([\"label\"], axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "print(f\"data shape {data.shape}\")\n",
    "print(f\"input feature shape {X.shape}\")\n",
    "print(f\"label (=target) shape {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 739525 training samples with 137877 signal and 601648 background events\n",
      "We have 364245 testing samples with 68190 signal and 296055 background events\n"
     ]
    }
   ],
   "source": [
    "signal = sig.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_evt_tag\", \"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "bkg1 = bk1.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_evt_tag\",\"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "bkg2 = bk2.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_evt_tag\",\"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = sklearn.model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"We have {len(X_train_orig)} training samples with {sum(y_train_orig)} signal and {sum(1-y_train_orig)} background events\")\n",
    "print(f\"We have {len(X_test_orig)} testing samples with {sum(y_test_orig)} signal and {sum(1-y_test_orig)} background events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_conv = X_train_orig.copy()\n",
    "y_train_conv = y_train_orig.copy()\n",
    "X_test_conv = X_test_orig.copy()\n",
    "y_test_conv = y_test_orig.copy()\n",
    "\n",
    "X_train = X_train_orig.copy()\n",
    "y_train = y_train_orig.copy()\n",
    "X_test = X_test_orig.copy()\n",
    "y_test = y_test_orig.copy()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler, # maxAbs\n",
    "    MinMaxScaler, # MinMax\n",
    "    Normalizer, # Normalization (equal integral)\n",
    "    StandardScaler# standard scaling\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the input features and the target variable\n",
    "for column in X_train.columns:\n",
    "    scaler = StandardScaler().fit(X_train.filter([column], axis=1))\n",
    "    X_train[column] = scaler.transform(X_train.filter([column], axis=1))\n",
    "    X_test[column] = scaler.transform(X_test.filter([column], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks we will use `pytorch`, a backend designed natively for tensor operations.\n",
    "I prefer it to tensorflow, because it exposes (i.e. you have to call them explicitly in your code) the optimizer steps and the backpropagation steps.\n",
    "\n",
    "You could also use the `tensorflow` backend, either directly or through the `keras` frontend.\n",
    "Saying \"I use keras\" does not tell you which backend is being used. It used to be either `tensorflow` or `theano`. Nowadays `keras` is I think almost embedded inside tensorflow, but it is still good to specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch` handles the data management via the `Dataset` and `DataLoader` classes.\n",
    "Here we don't need any specific `Dataset` class, because we are not doing sophisticated things, but you may need that in the future.\n",
    "\n",
    "The `Dataloader` class takes care of providing quick access to the data by sampling batches that are then fed to the network for (mini)batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([512, 23])\n",
      "Labels batch shape: torch.Size([512])\n",
      "tensor([[-0.5914, -0.7561,  0.4747,  ...,  0.9550,  1.7093,  0.9943],\n",
      "        [-0.2150,  0.5016,  1.0378,  ...,  0.9573,  1.6075, -0.5273],\n",
      "        [-0.1922, -0.3602,  0.8532,  ...,  0.9522, -0.4374, -1.0638],\n",
      "        ...,\n",
      "        [ 0.6061,  0.1150,  0.7471,  ...,  0.9538,  0.3880,  0.6489],\n",
      "        [ 1.0261, -0.6782, -0.8879,  ..., -1.0502,  0.0776,  1.4036],\n",
      "        [-0.7438, -0.7292,  0.3562,  ...,  0.9558, -0.8699,  1.4607]])\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, device=torch.device(\"cpu\")):\n",
    "        self.X = torch.Tensor(X.values if isinstance(X, pd.core.frame.DataFrame) else X).to(device)\n",
    "        self.y = torch.Tensor(y.values).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.y[idx]\n",
    "        datum = self.X[idx]\n",
    "        \n",
    "        return datum, label\n",
    "\n",
    "batch_size=512 # Minibatch learning\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For educational purposes, let's get access the data loader via its iterator, and sample a single batch by calling `next` on the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 23]) torch.Size([512])\n",
      "tensor([[ 0.5713, -0.0187, -0.8879,  ..., -1.0502, -0.2379, -0.1800],\n",
      "        [ 0.0549,  0.0262, -0.8879,  ..., -1.0502, -0.9357, -0.0690],\n",
      "        [-0.7902, -0.3524,  0.7317,  ...,  0.9547, -0.4274,  0.1844],\n",
      "        ...,\n",
      "        [-0.1079,  1.1026,  0.2895,  ...,  0.9464, -0.6725,  0.7612],\n",
      "        [ 2.4079,  1.6668, -0.1550,  ...,  0.9570,  1.4739,  1.2887],\n",
      "        [-0.6878, -0.7884, -0.8879,  ..., -1.0502, -0.7064, -1.3723]])\n"
     ]
    }
   ],
   "source": [
    "random_batch_X, random_batch_y = next(iter(train_dataloader))\n",
    "print(random_batch_X.shape, random_batch_y.shape) \n",
    "print(random_batch_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple neural network, by inheriting from the `nn.Module` class. **This is very crucial, because that class is the responsible for providing the automatic differentiation infrastructure for tracking parameters and doing backpropagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, ninputs, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(ninputs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64,8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "        self.device=device\n",
    "        self.linear_relu_stack.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        y = self.linear_relu_stack(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the neural network and print some info on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=23, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "    (7): Sigmoid()\n",
      "    (8): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Now let's see some more detailed info by using the torchinfo package\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NeuralNetwork                            [512, 1]                  --\n",
       "├─Sequential: 1-1                        [512, 1]                  --\n",
       "│    └─Linear: 2-1                       [512, 512]                12,288\n",
       "│    └─ReLU: 2-2                         [512, 512]                --\n",
       "│    └─Linear: 2-3                       [512, 128]                65,664\n",
       "│    └─ReLU: 2-4                         [512, 128]                --\n",
       "│    └─Linear: 2-5                       [512, 64]                 8,256\n",
       "│    └─ReLU: 2-6                         [512, 64]                 --\n",
       "│    └─Linear: 2-7                       [512, 8]                  520\n",
       "│    └─Sigmoid: 2-8                      [512, 8]                  --\n",
       "│    └─Linear: 2-9                       [512, 1]                  9\n",
       "==========================================================================================\n",
       "Total params: 86,737\n",
       "Trainable params: 86,737\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 44.41\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 2.92\n",
       "Params size (MB): 0.35\n",
       "Estimated Total Size (MB): 3.31\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(X_train.shape[1])\n",
    "\n",
    "print(model) # some basic info\n",
    "\n",
    "print(\"Now let's see some more detailed info by using the torchinfo package\")\n",
    "torchinfo.summary(model, input_size=(batch_size, X_train.shape[1])) # the input size is (batch size, number of features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's introduce a crucial concept: `torch` lets you manage in which device you want to put your data and models, to optimize access at different stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataloader resides in -1\n",
      "The new dataloader puts the batches in in 0\n"
     ]
    }
   ],
   "source": [
    "devicestring = \"mps\" # \"mps\" for macos. \"cuda\" for CUDA gpus, \"cpu\" for CPUs\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else devicestring)\n",
    "\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "random_batch_X, random_batch_y = next(iter(train_dataloader))\n",
    "\n",
    "print(\"The original dataloader resides in\", random_batch_X.get_device())\n",
    "\n",
    "# Let's reinstantiate the dataset\n",
    "device = torch.device(\"mps\")\n",
    "train_dataset = MyDataset(X_train, y_train, device=device)\n",
    "test_dataset = MyDataset(X_test, y_test, device=device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "random_batch_X, random_batch_y = next(iter(train_dataloader))\n",
    "\n",
    "print(\"The new dataloader puts the batches in in\", random_batch_X.get_device())\n",
    "\n",
    "# Reinstantiate the model, on the chosen device\n",
    "model = NeuralNetwork(X_train.shape[1], device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned how load the data into the GPU, how to define and instantiate a model. Now we need to define a training loop.\n",
    "\n",
    "In `keras`, this is wrapped hidden into the `.fit()` method, which I think is bad because it hides the actual procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    losses=[] # Track the loss function\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    #for batch, (X, y) in enumerate(dataloader):\n",
    "    for (X,y) in tqdm(dataloader):\n",
    "        # Reset gradients (to avoid their accumulation)\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction and loss\n",
    "        yhat = model(X)\n",
    "        #if (all_equal3(pred.detach().numpy())):\n",
    "        #    print(\"All equal!\")\n",
    "        loss = loss_fn(yhat.squeeze(dim=1), y)\n",
    "        losses.append(loss.detach().cpu())\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the loop that is run on the test dataset.\n",
    "\n",
    "**The test dataset is just used for evaluating the output of the model. No backpropagation is needed, therefore backpropagation must be switched off!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    losses=[] # Track the loss function\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        #for X, y in dataloader:\n",
    "        for (X,y) in tqdm(dataloader):\n",
    "            yhat = model(X)\n",
    "            loss = loss_fn(yhat.squeeze(dim=1), y).item()\n",
    "            losses.append(loss)\n",
    "            test_loss += loss\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now read to train this!\n",
    "At the moment we are trying to do classification. We will set our loss function to be the cross entropy.\n",
    "\n",
    "Torch provides the functionality to use generic functions as loss function. We will show an example one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loss_fn = torch.nn.MSELoss()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "def my_simple_loss(y_hat,y):\n",
    "    loss = torch.mean( y[:,0]*torch.pow( y_hat - y[:,1], 2))\n",
    "    #quad=-1,2\n",
    "    #lin=-2,1\n",
    "    #sm=-3,0\n",
    "    return loss\n",
    "# We would use this loss function in the same way as the other predefined loss functions:\n",
    "# loss_fn=my_simple_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to define optimizer and scheduler, number of epochs, and finally to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 83.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 109.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 546.7981 , Avg test loss 546.5809952382291 Current learning rate [0.009000000000000001]\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 93.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 102.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 543.103 , Avg test loss 544.8162519905004 Current learning rate [0.008100000000000001]\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 93.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 108.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 542.1319 , Avg test loss 544.4324509267057 Current learning rate [0.007290000000000001]\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 95.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 108.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 541.51447 , Avg test loss 543.6859098284432 Current learning rate [0.006561000000000002]\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 95.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 105.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 541.01 , Avg test loss 543.5680770874023 Current learning rate [0.005904900000000002]\n",
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 94.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 108.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 540.5649 , Avg test loss 543.4360286412614 Current learning rate [0.005314410000000002]\n",
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 91.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 99.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 540.2056 , Avg test loss 543.440261712235 Current learning rate [0.004782969000000002]\n",
      "Epoch 8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 90.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 101.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 539.84625 , Avg test loss 542.8040116556575 Current learning rate [0.004304672100000002]\n",
      "Epoch 9\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 86.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 105.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 539.4827 , Avg test loss 542.8814953150375 Current learning rate [0.003874204890000002]\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:18<00:00, 78.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 94.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 539.0323 , Avg test loss 542.8046495673361 Current learning rate [0.003486784401000002]\n",
      "Epoch 11\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 80.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 98.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 538.56775 , Avg test loss 542.4050886604223 Current learning rate [0.003138105960900002]\n",
      "Epoch 12\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 87.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 96.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 538.1754 , Avg test loss 542.6108638892013 Current learning rate [0.0028242953648100018]\n",
      "Epoch 13\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 88.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 101.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 537.80273 , Avg test loss 542.5310591794132 Current learning rate [0.0025418658283290017]\n",
      "Epoch 14\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 89.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 97.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 537.4641 , Avg test loss 542.1332623771067 Current learning rate [0.0022876792454961017]\n",
      "Epoch 15\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 90.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 95.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 537.0542 , Avg test loss 542.2444289132451 Current learning rate [0.0020589113209464917]\n",
      "Epoch 16\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 83.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 96.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 536.7482 , Avg test loss 542.4911910282092 Current learning rate [0.0018530201888518425]\n",
      "Epoch 17\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 83.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 91.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 536.4116 , Avg test loss 542.3385513177079 Current learning rate [0.0016677181699666583]\n",
      "Epoch 18\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 83.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 89.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 536.1089 , Avg test loss 542.221906169077 Current learning rate [0.0015009463529699924]\n",
      "Epoch 19\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 82.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 94.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 535.7703 , Avg test loss 542.5446583822871 Current learning rate [0.0013508517176729932]\n",
      "Epoch 20\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 83.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 94.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 535.44824 , Avg test loss 542.3954118021419 Current learning rate [0.001215766545905694]\n",
      "Epoch 21\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 82.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 96.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 535.1578 , Avg test loss 542.8031699148456 Current learning rate [0.0010941898913151245]\n",
      "Epoch 22\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 84.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 97.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 534.8743 , Avg test loss 542.9594008627902 Current learning rate [0.0009847709021836122]\n",
      "Epoch 23\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 87.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 94.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 534.5556 , Avg test loss 542.7676888369442 Current learning rate [0.0008862938119652509]\n",
      "Epoch 24\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 83.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 98.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 534.225 , Avg test loss 543.047944101055 Current learning rate [0.0007976644307687258]\n",
      "Epoch 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 88.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 101.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 533.98553 , Avg test loss 543.1055585025401 Current learning rate [0.0007178979876918532]\n",
      "Epoch 26\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 86.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 99.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 533.70435 , Avg test loss 543.3742289596729 Current learning rate [0.0006461081889226679]\n",
      "Epoch 27\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 85.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 94.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 533.37585 , Avg test loss 543.3127286675272 Current learning rate [0.0005814973700304011]\n",
      "Epoch 28\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 89.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 95.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 533.11096 , Avg test loss 544.0089394215787 Current learning rate [0.0005233476330273611]\n",
      "Epoch 29\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 85.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 103.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 532.8641 , Avg test loss 543.8077233989587 Current learning rate [0.000471012869724625]\n",
      "Epoch 30\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 91.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 102.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 532.61194 , Avg test loss 544.2036947614691 Current learning rate [0.0004239115827521625]\n",
      "Epoch 31\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 91.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 101.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 532.36127 , Avg test loss 544.1583689357458 Current learning rate [0.00038152042447694626]\n",
      "Epoch 32\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 91.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 101.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 532.1179 , Avg test loss 543.957093613871 Current learning rate [0.00034336838202925164]\n",
      "Epoch 33\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 91.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 102.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 531.9 , Avg test loss 544.6785161736307 Current learning rate [0.0003090315438263265]\n",
      "Epoch 34\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:16<00:00, 86.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:06<00:00, 102.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 531.62274 , Avg test loss 544.5943214759398 Current learning rate [0.00027812838944369386]\n",
      "Epoch 35\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:15<00:00, 91.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 97.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 531.4303 , Avg test loss 544.923731728886 Current learning rate [0.0002503155504993245]\n",
      "Epoch 36\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 82.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 94.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 531.18414 , Avg test loss 544.7267075442196 Current learning rate [0.00022528399544939206]\n",
      "Epoch 37\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 82.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 91.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 531.0202 , Avg test loss 545.2515291578314 Current learning rate [0.00020275559590445286]\n",
      "Epoch 38\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 82.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 100.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 530.83734 , Avg test loss 545.0631717296129 Current learning rate [0.00018248003631400757]\n",
      "Epoch 39\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 84.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 95.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 530.6658 , Avg test loss 545.5471105897025 Current learning rate [0.00016423203268260683]\n",
      "Epoch 40\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1445/1445 [00:17<00:00, 83.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 712/712 [00:07<00:00, 95.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 530.4839 , Avg test loss 545.3251035668877 Current learning rate [0.00014780882941434616]\n",
      "Epoch 41\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████████████████████████████▎                                                                                                                          | 402/1445 [00:04<00:15, 68.63it/s]"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "learningRate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader, model, loss_fn, optimizer, scheduler, device)\n",
    "    test_loss=test_loop(test_dataloader, model, loss_fn, device)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "plt.plot(train_losses, label=\"Average training loss\")\n",
    "plt.plot(test_losses, label=\"Average test loss\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rocs(scores_labels_names):\n",
    "    plt.figure()\n",
    "    for score, label, name  in scores_labels_names:\n",
    "        fpr, tpr, thresholds = roc_curve(label, score)\n",
    "        plt.plot(\n",
    "            fpr, tpr, \n",
    "            linewidth=2, \n",
    "            label=f\"{name} (AUC = {100.*auc(fpr, tpr): .2f} %)\"\n",
    "        )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "    plt.grid()\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_rocs([\n",
    "    (model(torch.tensor(X_train.to_numpy(),device=model.device)).numpy(force=True), y_train, \"Train\"), \n",
    "    (model(torch.tensor(X_test.to_numpy(),device=model.device)).numpy(force=True), y_test, \"Test\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHOOPS! The network is not learning anything!!!\n",
    "\n",
    "What can we do?\n",
    "\n",
    "Go back to that cell that had this text: `# LABEL OF THIS PLACE HERE, WILL BE USEFUL LATER`\n",
    "and add there the following lines:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# NOTE: in earlier versions of the StandardScaler, `.values.reshape(-1,1)` was not needed. The interface must have changed.\n",
    "\n",
    "for column in X_train.columns:\n",
    "    scaler = StandardScaler().fit(X_train[column].values.reshape(-1,1))\n",
    "    X_train[column] = scaler.transform(X_train[column].values.reshape(-1,1))\n",
    "    X_test[column] = scaler.transform(X_test[column].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also use the basic syntax recommended by the documentation, as follows, but then you would be standardizing all the features to exacly the same mean. This may work for some data sets, but for this specific one it does not (it actually significantly worsens the performance---you can try ;) ).\n",
    "\n",
    "```\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train[X_train.columns] = scaler.transform(X_train[X_train.columns])\n",
    "X_test[X_test.columns] = scaler.transform(X_test[X_test.columns])\n",
    "```\n",
    "\n",
    "Rerun starting from that cell, and now check the new loss function evolution.\n",
    "\n",
    "#### Can you explain what is the effect of these lines and their effect on the gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHOOPS! The network is not learning anything!!!\n",
    "\n",
    "What can we do?\n",
    "\n",
    "This new behaviour is likely caused by a too large value of initial learning rate, that makes training and test sink into two different minima. You can fix it by, for instance, reducing by a factor 10 the initial learning rate above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass\n",
    "\n",
    "Go back to the original dataset, but now assign different labels to the two backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = sig.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_evt_tag\", \"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "bkg1 = bk1.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_evt_tag\",\"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "bkg2 = bk2.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_evt_tag\",\"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "\n",
    "signal['label'] = 2\n",
    "bkg1['label'] = 1\n",
    "bkg2['label'] = 0\n",
    "bkg = pd.concat([bkg1, bkg2])\n",
    "\n",
    "data = pd.concat([signal,bkg]).sample(frac=1).reset_index(drop=True)\n",
    "X = data.drop([\"label\"], axis=1)\n",
    "y = data[\"label\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to apply the technique of **one-hot encoding** to convert a categorical label into a vector (one dimension per category/class).\n",
    "\n",
    "Now you need to apply the technique of **one-hot encoding** to convert a categorical label (=0,1,2) into a vector (one dimension per category/class):\n",
    "\n",
    "\n",
    "| Sample | Categorical | One-hot encoding |\n",
    "| --- | --- | --- |\n",
    "| Background 2 | $0$ | $(1,0,0)$ |\n",
    "| Background 1 | $1$ | $(0,1,0)$ |\n",
    "| Signal | $2$ | $(0,0,1)$ |\n",
    "\n",
    "\n",
    "You can use the function `one_hot = torch.nn.functional.one_hot(target)` to one-hot encode the target labels (both for signal and background)\n",
    "\n",
    "One-hot encoding is described in slide `81`of this morning's lecture.\n",
    "\n",
    "You can use the function `one_hot = torch.nn.functional.one_hot(target)` to one-hot encode the target labels (both for signal and background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this where appropriate\n",
    "for label in [0,1,2]:\n",
    "    one_hot = torch.nn.functional.one_hot(torch.tensor(label), num_classes=3)\n",
    "    print (f\"Encoding label '{label}' as '{one_hot.numpy(force=True)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to recreate the test/train split, using the same lines of code you used for the simple classification problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you have to modify the neural network model: the output must be a dimension-three vector rather than a scalar.\n",
    "\n",
    "You can use `self.output = nn.Linear(8, 3)` as last layer, and you can prepend it a `SoftMax` or `sigmoid` activation function, to ensure the outputs are interpretable as probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you will need to figure out how to get categorical predictions to be able to test performance (for instance to produce the ROC curve for each pair of classes, or for one class against all the others.\n",
    "\n",
    "However, the minimal useful thing is to produce the confusion matrix we have seen in this morning's lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "pred_y = model(torch.tensor(X_test.values, device=device)).numpy(force=True)\n",
    "pred_class = np.argmax(pred_y,axis=1)\n",
    "true_class = np.argmax(y_test.numpy(force=True),axis=1)\n",
    "\n",
    "print (f\"{'true class':10s} | {'predicted class':15s} \")\n",
    "print ('='*30)\n",
    "for i in range(10):\n",
    "    print (f\"{true_class[i]:<10d} | {pred_class[i]:<15d}\")\n",
    "\n",
    "confusion_mat = confusion_matrix(true_class, pred_class, normalize='true')\n",
    "                                 \n",
    "plt.figure()\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=['bkg2', 'bkg1', 'sig'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Calculate the predictions for each class, then plot the ROC curve for:\n",
    "\n",
    "- signal vs bkg1\n",
    "- signal vs bkg2\n",
    "- bkg vs bkg2\n",
    "\n",
    "Then, in another plot, plot:\n",
    "\n",
    "- signal vs all backgrounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Go back to the original dataset, but now we will use the Higgs boson transverse momentum as a target for regression.\n",
    "Notice how we avoid dropping the `Hreco_HTXS_Higgs_pt` column from the dataset, and we put that one into the target `y`.\n",
    "\n",
    "The training will be done only on the signal (you want to regress the momentum in the specific process of interest).\n",
    "\n",
    "We will still use the backgrounds, but just to make comparisons, in the sense that once you have the pT regressor, you can apply it to ttH (signal) events, but also separately to background events to see what's the shape of the regressed pT where the regressed pT is not supposed to exist (Drell-Yan events) or when it is supposed to exist but for another particle (W boson in ttW) events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = sig.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_evt_tag\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "bkg1 = bk1.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_evt_tag\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "bkg2 = bk2.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_evt_tag\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n",
    "\n",
    "X = signal.drop([\"Hreco_HTXS_Higgs_pt\"], axis=1)\n",
    "y = signal[\"Hreco_HTXS_Higgs_pt\"]\n",
    "\n",
    "`# MIMIMI HERE SOMETHING THERE WILL BE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since the target is a regression, we need to tweak two things.\n",
    "\n",
    "First, the last activation function should not be a `nn.Sigmoid()` anymore (which forces the output to be in the range `[0,1]`. You should now use `nn.ReLU()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the cross entropy loss is not appropriate anymore. You should use the `MSELoss()`.\n",
    "\n",
    "With these two changes, you should be able to regress the Higgs boson transverse momentum.\n",
    "\n",
    "Plot the loss function, and then produce a scatter plot of the neural network prediction versus the true value of the Higgs transverse momentum (`y` vs `pred=model(x)`). Finally, produce a plot where you show the shape of the pT regressor separately for the signal, for bkg1 (ttW),  and for bkg2 (Drell-Yan). For this latest plot, you should normalize to 1 the three distributions, to check for shape differences (you can use `density=True` when plotting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is going on!?!??! Why is the loss always NotANumber?\n",
    "\n",
    "This is because the network is not managing to cope with the vast range of values for the output (the pT).\n",
    "\n",
    "Try reducing the range of values by adding, in correspondence of `# MIMIMI HERE SOMETHING THERE WILL BE`, the following transformation:\n",
    "\n",
    "\n",
    "`y = signal[\"Hreco_HTXS_Higgs_pt\"].apply(np.log)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Somehow better, but still suboptimal!\n",
    "\n",
    "The spread of the predictions is too large to be used. Nonconvex optimization is difficult, and sometimes tweaking the model and training to success is tricky.\n",
    "\n",
    "A way of hacking this problem is to use a more sophisticated loss function that penalizes predictions with different means. You can try!\n",
    "\n",
    "```\n",
    "class penalized_mse(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        #return ((pred-target)**2).mean() + 2*((torch.log(pred)-torch.log(target))**2).mean()\n",
    "        print(pred.mean(), pred.var(),target.var())\n",
    "        return ((pred-target)**2).mean()*(torch.abs(pred.var()-target.var()))\n",
    "\n",
    "loss_fn=penalized_mse()\n",
    "```\n",
    "\n",
    "Also see if modifying the network can help to improve the prediction. For example ...\n",
    "* add dropout layers\n",
    "* use a different activation function\n",
    "* add batch normalization layers\n",
    "* reduce the number of layer\n",
    "* reduce the number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
