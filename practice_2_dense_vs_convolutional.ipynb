{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06eb7b0",
   "metadata": {},
   "source": [
    "# Analisis de Datos en FÃ­sica Moderna\n",
    "## Pietro Vischia (Universidad de Oviedo and ICTEA), pietro.vischia@cern.ch\n",
    "\n",
    "The core of this tutorial comes from https://github.com/vischia/data_science_school_igfae2024 (Pietro Vischia (pietro.vischia@cern.ch))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e1b03",
   "metadata": {},
   "source": [
    "Load the MNIST dataset and inspect it.\n",
    "This dataset is handily hosted in the package `torchvision`.\n",
    "\n",
    "You can also apply a series of transformations, e.g. the following if you want to standardize automatically the dataset to the global mean and variance. Suggestion: start by not standardizing.\n",
    "\n",
    "NOTE: the standardization transform is called `normalize` in the package `torchvision`.\n",
    "\n",
    "```\n",
    "transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "```\n",
    "\n",
    "Another transform it is usually done is to divide values in the image by `255` to have them before 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # Or whichever value you may see fit\n",
    "# You can also explore whether dense and convolutional networks work best with different batch sizes\n",
    "\n",
    "# Load Data\n",
    "train_dataset = torchvision.datasets.MNIST(root='dataset/', train=True, transform=torchvision.transforms.ToTensor(),  target_transform = torchvision.transforms.Compose([\n",
    "                                 lambda x:torch.LongTensor([x]), # or just torch.tensor\n",
    "                                 lambda x:F.one_hot(x,10)]), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='dataset/', train=False, transform=torchvision.transforms.ToTensor(), \n",
    "target_transform = torchvision.transforms.Compose([\n",
    "                                 lambda x:torch.LongTensor([x]), # or just torch.tensor\n",
    "                                 lambda x:F.one_hot(x,10)]),                                          download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed3e07",
   "metadata": {},
   "source": [
    "We can inspect the dataset.\n",
    "\n",
    "Suggestion: inspect the dataset after trying out different transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "example_data.shape\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375adbef",
   "metadata": {},
   "source": [
    "Define a fully connected neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Sequential(\n",
    "            # here put layers. Some choices you have seen are:\n",
    "            #     nn.Dropout(p=0.4),\n",
    "            #     nn.Linear(input_size, 128),\n",
    "            #     nn.BatchNorm1d(128),\n",
    "            #     nn.ReLU(),\n",
    "            #     nn.sigmoid(),\n",
    "            #     nn.Softmax(dim=1)\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "        self.dense.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28*1 # You have to figure this out\n",
    "num_classes = 10 # You have to figure this out\n",
    "learning_rate = 0.001 # You have to figure this out\n",
    "num_epochs = 3 # You have to figure this out\n",
    "\n",
    "# Initialize Network\n",
    "model = DenseNetwork(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # You have to figure this out\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd78a72",
   "metadata": {},
   "source": [
    "The transformation `torchvision.transforms.Normalize` does what we call *Standardizatio*, that is it rescales data to have the same mean and variance. Here, the two hardcoded values are the mean and variance of the whole MNIST data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, cnn, device):\n",
    "    losses=[] # Track the loss function\n",
    "    accs= [] # Track accuracies\n",
    "    \n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "    #for batch, (X, y) in tqdm(enumerate(dataloader, 0), unit=\"batch\", total=len(dataloader)):\n",
    "    #for (X,y) in tqdm(dataloader):\n",
    "        # Reset gradients (to avoid their accumulation)\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction and loss\n",
    "        if not cnn:\n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "        pred = model(X)\n",
    "        pred= pred.reshape([pred.shape[0], 1, num_classes])\n",
    "        loss = loss_fn(pred, y.float())\n",
    "        losses.append(loss.detach().cpu())\n",
    "        acc = (torch.argmax(pred, 1) == torch.argmax(y, 1)).float().mean()\n",
    "        accs.append(acc.detach().cpu())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(losses), np.mean(accs)\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, cnn, device):\n",
    "\n",
    "    losses=[] # Track the loss function\n",
    "    accs = [] # Track accuracies\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        #for batch, (X, y) in tqdm(enumerate(dataloader, 0), unit=\"batch\", total=len(dataloader)):\n",
    "        for (X,y) in tqdm(dataloader):\n",
    "            if not cnn:\n",
    "                X = X.reshape(X.shape[0], -1)\n",
    "            pred = model(X)\n",
    "            pred= pred.reshape([pred.shape[0], 1, num_classes])\n",
    "            loss = loss_fn(pred, y.float())\n",
    "            acc = (torch.argmax(pred, 1) == torch.argmax(y, 1)).float().mean()\n",
    "            losses.append(loss)\n",
    "            accs.append(acc.detach().cpu())\n",
    "            test_loss += loss\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    return np.mean(losses), np.mean(accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "test_losses=[]\n",
    "train_accuracies=[]\n",
    "test_accuracies=[]\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss, train_accuracy = train_loop(train_loader, model, loss_fn, optimizer, scheduler, False, device)\n",
    "    test_loss, test_accuracy = test_loop(test_loader, model, loss_fn, False, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e385b3",
   "metadata": {},
   "source": [
    "Now let's create a convolutional network\n",
    "\n",
    "Relevant parameters:\n",
    "\n",
    "- `in_channels`: number of input channels. For black-and-white images, this is 1. RGB images have 3.\n",
    "- `num_classes`: how many digits do we want to recognize?\n",
    "\n",
    "The first `Conv2d` layer has 8 kernels of size 3x3, i.e. splits the image in eight separate channels each with its own convolution operation. Padding ensures the size of the image remains the same.\n",
    "The second `Conv2d` layer has 16 filters, also with 3x3 kernel.\n",
    "\n",
    "The `MaxPool2d` layer has a 2x2 kernel and a stride of 2. This does averaging and dimensional reduction, downsampling the image by a factor 2 in each dimension (from 28x28 to 14x14 the first time we apply it, and from 14x14 to 7x7 the second time).\n",
    "\n",
    "The output of the second `Conv2d` layer will therefore a 16-channel image where each channel is 7x7. We flatten it to a one-dimensional vector per image to feed it to a dense layer that does classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f7dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(ConvNetwork, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*7*7, num_classes)\n",
    "            # Another activation function here? Or not?\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab228d6",
   "metadata": {},
   "source": [
    "Now we change the input parameters accordingly (in general they won't be the same as the dense network) and instatiate the convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910fa8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1 # You have to figure this out\n",
    "num_classes = 10 # You have to figure this out\n",
    "learning_rate = 0.01 # You have to figure this out\n",
    "num_epochs = 5 # You have to figure this out\n",
    "\n",
    "# Initialize Network\n",
    "model = ConvNetwork(in_channels=in_channels, num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss() # You have to figure this out\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892439e",
   "metadata": {},
   "source": [
    "Now you can train again. The training and test look will be the same, but you will need to change the names in the loop below to save the training data for the convolutional network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb593af",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_train_losses=[]\n",
    "conv_test_losses=[]\n",
    "conv_train_accuracies=[]\n",
    "conv_test_accuracies=[]\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    conv_train_loss, conv_train_accuracy = train_loop(train_loader, model, loss_fn, optimizer, scheduler, True, device)\n",
    "    conv_test_loss, conv_test_accuracy = test_loop(test_loader, model, loss_fn, True, device)\n",
    "    conv_train_losses.append(conv_train_loss)\n",
    "    conv_train_accuracies.append(conv_train_accuracy)\n",
    "    conv_test_losses.append(conv_test_loss)\n",
    "    conv_test_accuracies.append(conv_test_accuracy)\n",
    "    print(\"Avg train loss\", conv_train_loss, \", Avg test loss\", conv_test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2af65",
   "metadata": {},
   "source": [
    "Now you can for instance plot the losses for both networks (train and test data sets), build the confusion matrix for the two networks, check how many trainable parameters (remember `torchinfo.summary(model)`...) are needed for each network type to give you a certain performance, and other useful stuff for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b087c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
