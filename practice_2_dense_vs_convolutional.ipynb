{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ff66f4",
   "metadata": {},
   "source": [
    "# Analisis de Datos en FÃ­sica Moderna\n",
    "## Pietro Vischia (Universidad de Oviedo and ICTEA), pietro.vischia@cern.ch\n",
    "\n",
    "The core of this tutorial comes from https://github.com/vischia/data_science_school_igfae2024 (Pietro Vischia (pietro.vischia@cern.ch))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97136cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75274157",
   "metadata": {},
   "source": [
    "Load the MNIST dataset and inspect it.\n",
    "This dataset is handily hosted in the package `torchvision`.\n",
    "\n",
    "You can also apply a series of transformations, e.g. the following if you want to standardize automatically the dataset to the global mean and variance. Suggestion: start by not standardizing.\n",
    "\n",
    "NOTE: the standardization transform is called `normalize` in the package `torchvision`.\n",
    "\n",
    "```\n",
    "transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "```\n",
    "\n",
    "Another transform it is usually done is to divide values in the image by `255` to have them before 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292baefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # Or whichever value you may see fit\n",
    "# You can also explore whether dense and convolutional networks work best with different batch sizes\n",
    "\n",
    "# Load Data\n",
    "train_dataset = torchvision.datasets.MNIST(root='dataset/', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='dataset/', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685fff51",
   "metadata": {},
   "source": [
    "We can inspect the dataset.\n",
    "\n",
    "Suggestion: inspect the dataset after trying out different transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "example_data.shape\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce42c80",
   "metadata": {},
   "source": [
    "Define a fully connected neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab98387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetwork(nn.Module):\n",
    "    def __init__(self, ninputs, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Sequential(\n",
    "            # here put layers. Some choices you have seen are:\n",
    "            #     nn.Dropout(p=0.4),\n",
    "            #     nn.Linear(ninputs, 128),\n",
    "            #     nn.BatchNorm1d(128),\n",
    "            #     nn.ReLU(),\n",
    "            #     nn.sigmoid(),\n",
    "            #     nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.linear_relu_stack.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.dense(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = # You have to figure this out\n",
    "num_classes = # You have to figure this out\n",
    "learning_rate = # You have to figure this out\n",
    "num_epochs = # You have to figure this out\n",
    "\n",
    "# Initialize Network\n",
    "model = DenseNetwork(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = # You have to figure this out\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59919e06",
   "metadata": {},
   "source": [
    "The transformation `torchivision.transforms.Normalize` does what we call *Standardizatio*, that is it rescales data to have the same mean and variance. Here, the two hardcoded values are the mean and variance of the whole MNIST data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(...):\n",
    "\n",
    "    # From the suitable notebook\n",
    "    \n",
    "    return np.mean(losses), np.mean(accs)\n",
    "\n",
    "def test_loop(...):\n",
    "\n",
    "    # From the suitable notebook\n",
    "    return np.mean(losses), np.mean(accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a9a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss, train_accuracy = train_loop(train_dataloader, model, loss_fn, optimizer, scheduler, device)\n",
    "    test_loss, test_accuracy = test_loop(test_dataloader, model, loss_fn, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31e015",
   "metadata": {},
   "source": [
    "Now let's create a convolutional network\n",
    "\n",
    "Relevant parameters:\n",
    "\n",
    "- `in_channels`: number of input channels. For black-and-white images, this is 1. RGB images have 3.\n",
    "- `num_classes`: how many digits do we want to recognize?\n",
    "\n",
    "The first `Conv2d` layer has 8 kernels of size 3x3, i.e. splits the image in eight separate channels each with its own convolution operation. Padding ensures the size of the image remains the same.\n",
    "The second `Conv2d` layer has 16 filters, also with 3x3 kernel.\n",
    "\n",
    "The `MaxPool2d` layer has a 2x2 kernel and a stride of 2. This does averaging and dimensional reduction, downsampling the image by a factor 2 in each dimension (from 28x28 to 14x14 the first time we apply it, and from 14x14 to 7x7 the second time).\n",
    "\n",
    "The output of the second `Conv2d` layer will therefore a 16-channel image where each channel is 7x7. We flatten it to a one-dimensional vector per image to feed it to a dense layer that does classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4866cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*7*7, num_classes)\n",
    "            # Another activation function here? Or not?\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be98e75",
   "metadata": {},
   "source": [
    "Now we change the input parameters accordingly (in general they won't be the same as the dense network) and instatiate the convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = # You have to figure this out\n",
    "num_classes = # You have to figure this out\n",
    "learning_rate = # You have to figure this out\n",
    "num_epochs = # You have to figure this out\n",
    "\n",
    "# Initialize Network\n",
    "model = ConvNetwork(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = # You have to figure this out\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0238f0",
   "metadata": {},
   "source": [
    "Now you can train again. The training and test look will be the same, but you will need to change the names in the loop below to save the training data for the convolutional network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7dae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    conv_train_loss, conv_train_accuracy = train_loop(train_dataloader, model, loss_fn, optimizer, scheduler, device)\n",
    "    conv_test_loss, conv_test_accuracy = test_loop(test_dataloader, model, loss_fn, device)\n",
    "    conv_train_losses.append(conv_train_loss)\n",
    "    conv_train_accuracies.append(conv_train_accuracy)\n",
    "    conv_test_losses.append(conv_test_loss)\n",
    "    conv_test_accuracies.append(conv_test_accuracy)\n",
    "    print(\"Avg train loss\", conv_train_loss, \", Avg test loss\", conv_test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c37a41",
   "metadata": {},
   "source": [
    "Now you can for instance plot the losses for both networks (train and test data sets), build the confusion matrix for the two networks, check how many trainable parameters (remember `torchinfo.summary(model)`...) are needed for each network type to give you a certain performance, and other useful stuff for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cae408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
