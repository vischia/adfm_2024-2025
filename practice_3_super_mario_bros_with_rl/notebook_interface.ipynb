{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vischia/adfm_2024-2025/blob/master/practice_3_super_mario_bros_with_rl/notebook_interface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n",
    "\n",
    "Run this once in a fresh environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 gym==0.26.2 gym-super-mario-bros==7.4.0 nes-py==8.2.1 torch torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.bis: run this only if you need rendering in Google Colab! apt install freeglut3 freeglut3-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runOnColab=False\n",
    "\n",
    "if runOnColab:\n",
    "    ! apt install freeglut3 freeglut3-dev\n",
    "! pip install pyvirtualdisplay matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation\n",
    "\n",
    "\n",
    "def make_mario_env(world: int = 1, stage: int = 1):\n",
    "    \"\"\"\n",
    "    Create a Super Mario Bros environment with:\n",
    "      - Discrete SIMPLE_MOVEMENT action space.\n",
    "      - Grayscale 84x84 observations.\n",
    "    \"\"\"\n",
    "    env_id = f\"SuperMarioBros-{world}-{stage}-v0\"\n",
    "    env = gym_super_mario_bros.make(env_id, \n",
    "        apply_api_compatibility=True,\n",
    "        render_mode=\"rgb_array\")  # use \"human\" if running on your laptop.\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    env = GrayScaleObservation(env, keep_dim=False)  # (H, W)\n",
    "    env = ResizeObservation(env, 84)                 # (84, 84)\n",
    "    return env\n",
    "\n",
    "\n",
    "def preprocess_state(state, device):\n",
    "    \"\"\"\n",
    "    Flatten and normalize (84,84) grayscale image to (1, 84*84) tensor.\n",
    "    \"\"\"\n",
    "    state = np.array(state, dtype=np.float32) / 255.0\n",
    "    state = state.flatten()\n",
    "    return torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Simple feed-forward DQN with flattened image input.\"\"\"\n",
    "    def __init__(self, input_dim: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"Fixed-size replay buffer for experience replay.\"\"\"\n",
    "    def __init__(self, capacity: int):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.utils as nn_utils\n",
    "\n",
    "class MarioDQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        input_dim: int = 84 * 84,\n",
    "        lr: float = 1e-4,\n",
    "        gamma: float = 0.99,\n",
    "        replay_capacity: int = 50000,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.policy_net = DQN(input_dim, n_actions).to(device)\n",
    "        self.target_net = DQN(input_dim, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.memory = ReplayMemory(replay_capacity)\n",
    "\n",
    "    def select_action(self, state, epsilon: float) -> int:\n",
    "        \"\"\"STUDENT TODO: epsilon-greedy policy.\n",
    "\n",
    "        - With probability epsilon: return a random action in [0, n_actions - 1].\n",
    "        - Otherwise: return argmax_a Q(state, a) according to policy_net.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"select_action is not implemented\")\n",
    "\n",
    "    def optimize_model(self, batch_size: int):\n",
    "        \"\"\"STUDENT TODO: one DQN update step using replay memory.\n",
    "\n",
    "        Steps:\n",
    "            1. Return early if there are fewer transitions than batch_size.\n",
    "            2. Sample a batch of transitions (state, action, reward, next_state, done).\n",
    "            3. Compute current Q(s,a) and targets:\n",
    "                   target = r + gamma * max_a' Q_target(s', a') * (1 - done)\n",
    "            4. Compute Huber (smooth L1) loss and do a gradient step.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"optimize_model is not implemented\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "\n",
    "def run_agent_episode(agent: MarioDQNAgent, world: int = 1, stage: int = 1, render: bool = True) -> float:\n",
    "    env = make_mario_env(world, stage)\n",
    "    obs, info = env.reset()\n",
    "    prev_screen = env.render()\n",
    "    plt.imshow(prev_screen)\n",
    "    state = preprocess_state(obs, agent.device)\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        screen = env.render()\n",
    "        plt.imshow(screen)\n",
    "        action = agent.select_action(state, epsilon=0.0)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state = preprocess_state(next_obs, agent.device)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        ipythondisplay.clear_output(wait=True)\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    env.close()\n",
    "    print(f\"[Agent] Total reward: {total_reward:.2f}\")\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def run_random_episode(world: int = 1, stage: int = 1, render: bool = True) -> float:\n",
    "    env = make_mario_env(world, stage)\n",
    "    obs, info = env.reset()\n",
    "    prev_screen = env.render()\n",
    "    plt.imshow(prev_screen)\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        screen = env.render()\n",
    "        plt.imshow(screen)\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        ipythondisplay.clear_output(wait=True)\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    env.close()\n",
    "    print(f\"[Random] Total reward: {total_reward:.2f}\")\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_agent_vs_random(agent: MarioDQNAgent, episodes: int = 5):\n",
    "    agent_scores = []\n",
    "    random_scores = []\n",
    "    for i in range(episodes):\n",
    "        print(f\"\\n=== Match {i+1}/{episodes} ===\")\n",
    "        agent_scores.append(run_agent_episode(agent, render=False))\n",
    "        random_scores.append(run_random_episode(render=False))\n",
    "\n",
    "    print(\"\\n=== Summary (computer vs computer) ===\")\n",
    "    print(f\"Agent average reward:  {np.mean(agent_scores):.2f}\")\n",
    "    print(f\"Random average reward: {np.mean(random_scores):.2f}\")\n",
    "\n",
    "\n",
    "def run_human_episode(world: int = 1, stage: int = 1):\n",
    "    env = make_mario_env(world, stage)\n",
    "    obs, info = env.reset()\n",
    "    prev_screen = env.render()\n",
    "    plt.imshow(prev_screen)\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    print(\"\\nActions (SIMPLE_MOVEMENT):\")\n",
    "    for idx, action in enumerate(SIMPLE_MOVEMENT):\n",
    "        print(f\"{idx}: {action}\")\n",
    "    print(\"Press Ctrl+C to quit.\")\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        try:\n",
    "            a = int(input(\"Choose action index: \"))\n",
    "            if a < 0 or a >= len(SIMPLE_MOVEMENT):\n",
    "                print(\"Invalid index, try again.\")\n",
    "                continue\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid integer.\")\n",
    "            continue\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(a)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        ipythondisplay.clear_output(wait=True)\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "\n",
    "\n",
    "    pythondisplay.clear_output(wait=True)\n",
    "    env.close()\n",
    "    print(f\"[Human] Total reward: {total_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training infrastructure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(\n",
    "    num_episodes: int = 200,\n",
    "    batch_size: int = 32,\n",
    "    epsilon_start: float = 1.0,\n",
    "    epsilon_end: float = 0.1,\n",
    "    epsilon_decay_episodes: int = 150,\n",
    "    target_update_interval: int = 1000,\n",
    ") -> MarioDQNAgent:\n",
    "    \"\"\"STUDENT TODO: implement the full DQN training loop.\n",
    "\n",
    "    Suggested algorithm:\n",
    "        - Create env with make_mario_env()\n",
    "        - Instantiate MarioDQNAgent.\n",
    "        - For each episode:\n",
    "            * Reset env, preprocess initial state:\n",
    "                  obs = env.reset()\n",
    "                  state = preprocess_state(obs, device)\n",
    "            * Compute epsilon via linear decay:\n",
    "                  frac = min(episode_idx / epsilon_decay_episodes, 1.0)\n",
    "                  epsilon = epsilon_start + frac * (epsilon_end - epsilon_start)\n",
    "            * Loop until done:\n",
    "                - Choose action via agent.select_action(state, epsilon).\n",
    "                - Step env; get obs, reward, done, info.\n",
    "                - Preprocess next observation.\n",
    "                - Store transition in replay memory.\n",
    "                - Call agent.optimize_model(batch_size).\n",
    "                - Periodically update target_net.\n",
    "                - Accumulate total_reward.\n",
    "        - Close env and return the trained agent.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    env = make_mario_env()\n",
    "    n_actions = env.action_space.n\n",
    "    agent = MarioDQNAgent(n_actions=n_actions, device=device)\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # TODO: implement the main training loop as described above.\n",
    "        raise NotImplementedError(\"train_dqn main loop is not implemented\")\n",
    "\n",
    "    env.close()\n",
    "    return agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = train_dqn(num_episodes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate and play\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_human_episode()\n",
    "run_agent_episode(agent, render=True)\n",
    "evaluate_agent_vs_random(agent, episodes=2)\n",
    "# run_human_episode()  # uncomment to control Mario manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
